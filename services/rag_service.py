import asyncio
from typing import List, Optional, Dict, Any
from operator import itemgetter
from loguru import logger

# LangChain æ ¸å¿ƒç»„ä»¶
from langchain_core.runnables import Runnable, RunnablePassthrough, RunnableLambda, RunnableBranch
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.documents import Document
from langchain_core.vectorstores import VectorStoreRetriever 

from core.config import settings
from storage.vector_store import vector_store
from services.llm_service import llm_service     

class RAGService:
    def __init__(self):
        self.vector_store = vector_store
        self.llm = llm_service.langchain_llm
        self.collection: Optional[object] = None

    async def initialize(self):
        """åˆå§‹åŒ– RAG æœåŠ¡"""
        logger.info("âš™ï¸ æ­£åœ¨åˆå§‹åŒ– RAG Service...")
        await self.connect_milvus()
        
        if self.collection:
            try:
                self.collection.load()
                logger.info(f"âœ… Milvus Collection '{settings.COLLECTION_NAME}' å·²åŠ è½½åˆ°å†…å­˜")
            except Exception as e:
                logger.warning(f"âš ï¸ Milvus Collection åŠ è½½å¤±è´¥: {e}")
        
        try:
            _ = self.vector_store.embeddings
            logger.info("âœ… Embedding æ¨¡å‹å·²å°±ç»ª")
        except Exception as e:
            logger.error(f"âŒ Embedding æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    async def connect_milvus(self):
        await self.vector_store.connect_milvus()
        self.collection = self.vector_store.collection
    
    async def process_data(self, file_paths: List[str]):
        if not self.collection:
            await self.connect_milvus()

        logger.info(f"ğŸ“‚ å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        try:
            await self.vector_store.index_documents(file_paths=file_paths)
            if self.collection:
                self.collection.load()
            logger.success("âœ… æ–‡æ¡£ç´¢å¼•å®Œæˆå¹¶å·²ç”Ÿæ•ˆã€‚")
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            raise

    def get_retriever(self, user_id_card: Optional[str] = None) -> VectorStoreRetriever:
        return self.vector_store.get_retriever(user_id_card=user_id_card)

    def get_rag_chain(self, user_id_card: str) -> Runnable:
        
        retriever = self.get_retriever(user_id_card=user_id_card)

        # --- æ­¥éª¤ 1: å®šä¹‰ "é—®é¢˜æ”¹å†™" åˆ†æ”¯é€»è¾‘ ---
        
        # A. æ”¹å†™é—®é¢˜çš„ Prompt
        contextualize_q_system_prompt = (
            "ç»™å®šä¸€æ®µèŠå¤©è®°å½•å’Œç”¨æˆ·æœ€æ–°çš„é—®é¢˜ï¼ˆè¯¥é—®é¢˜å¯èƒ½å¼•ç”¨äº†ä¸Šä¸‹æ–‡ï¼‰ï¼Œ"
            "è¯·å°†è¯¥é—®é¢˜æ”¹å†™ä¸ºä¸€ä¸ªç‹¬ç«‹çš„ã€æ— éœ€ä¸Šä¸‹æ–‡å³å¯ç†è§£çš„å®Œæ•´é—®é¢˜ã€‚"
            "ä¸è¦å›ç­”é—®é¢˜ï¼Œåªéœ€è¿”å›æ”¹å†™åçš„é—®é¢˜ï¼›å¦‚æœæ— éœ€æ”¹å†™ï¼ŒåŸæ ·è¿”å›ã€‚"
        )
        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", contextualize_q_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{question}"),
            ]
        )
        
        # B. æ”¹å†™é“¾ (å¢åŠ  run_name="QuestionRewriter")
        rewrite_chain = (
            contextualize_q_prompt 
            | self.llm.with_config(run_name="QuestionRewriter") 
            | StrOutputParser()
        )

        # C. åˆ†æ”¯è·¯ç”±ï¼šæ— å†å²ç›´æ¥è¿”å›é—®é¢˜ï¼Œæœ‰å†å²åˆ™è°ƒç”¨æ”¹å†™é“¾
        query_transform_branch = RunnableBranch(
            (
                lambda x: not x.get("chat_history"),
                RunnableLambda(lambda x: x["question"])
            ),
            rewrite_chain
        )

        # D. ç»„åˆå†å²æ„ŸçŸ¥æ£€ç´¢å™¨
        history_aware_retriever = query_transform_branch | retriever

        # --- æ­¥éª¤ 2: å®šä¹‰ "å›ç­”ç”Ÿæˆ" é€»è¾‘ ---
        
        qa_system_template = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½åŠ©æ‰‹ã€‚\n"
            "è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„èƒŒæ™¯ä¿¡æ¯ (context) å›ç­”é—®é¢˜ã€‚\n"
            "å¦‚æœèƒŒæ™¯ä¿¡æ¯é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´â€œç”±äºç¼ºä¹ç›¸å…³ä¿¡æ¯ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜â€ï¼Œä¸è¦ç¼–é€ ã€‚\n"
            "å›ç­”è¦æ¡ç†æ¸…æ™°ï¼Œä½¿ç”¨ Markdown æ ¼å¼ã€‚\n\n"
            "èƒŒæ™¯ä¿¡æ¯:\n"
            "{context}"
        )
        
        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", qa_system_template),
                MessagesPlaceholder("chat_history"),
                ("human", "{question}"),
            ]
        )

        def format_docs(docs: List[Document]) -> str:
            return "\n\n".join(f"[èµ„æ–™ç‰‡æ®µ] {doc.page_content}" for doc in docs)

        # --- æ­¥éª¤ 3: ç»„è£…æ£€ç´¢é“¾ (ä½ ä¹‹å‰å¯èƒ½æ¼æ‰äº†è¿™ä¸ªå˜é‡çš„å®šä¹‰) ---
        
        retrieval_chain = RunnablePassthrough.assign(
            docs=history_aware_retriever,
        ).assign(
            context=lambda x: format_docs(x["docs"]),
            sources=lambda x: x["docs"]
        )

        # --- æ­¥éª¤ 4: ç»„è£…æœ€ç»ˆ RAG é“¾ ---

        rag_chain = (
            retrieval_chain
            | RunnablePassthrough.assign(
                # å¢åŠ  run_name="AnswerGenerator" ä»¥ä¾¿ API å±‚è¿‡æ»¤
                answer=qa_prompt 
                       | self.llm.with_config(run_name="AnswerGenerator") 
                       | StrOutputParser()
            )
        )
        
        return rag_chain

rag_service = RAGService()