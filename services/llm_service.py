import time
import json
import httpx
from loguru import logger
from typing import Dict, Any, List, Optional, AsyncIterator
from core.config import settings
from models.api_models import SimpleChatResponse
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.outputs import ChatGenerationChunk, ChatResult



class VLLMChatModel(BaseChatModel):

    llm_service: Any # æ¥æ”¶ LLMService å®ä¾‹ï¼Œç”¨äºè°ƒç”¨ chat_stream

    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Any = None, **kwargs: Any) -> ChatResult:
        """åŒæ­¥ç”Ÿæˆæ–¹æ³•ï¼Œä¸æ¨èåœ¨æµå¼åº”ç”¨ä¸­ä½¿ç”¨ã€‚"""
        # å¼ºåˆ¶è¦æ±‚ä½¿ç”¨å¼‚æ­¥æµå¼æ–¹æ³•
        raise NotImplementedError("è¯·ä½¿ç”¨å¼‚æ­¥æ–¹æ³• _agenerate æˆ–æµå¼æ–¹æ³• _astream")

    async def _astream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Any = None,
        **kwargs: Any,
    ) -> AsyncIterator[ChatGenerationChunk]:
        """å¼‚æ­¥æµå¼ç”Ÿæˆæ–¹æ³•ï¼Œè°ƒç”¨åº•å±‚ LLMService çš„æµå¼ APIã€‚"""
        
        # æå–ç”¨æˆ·æ¶ˆæ¯å’Œå†å²è®°å½•ï¼Œé€‚åº” LLMService çš„ _build_messages æ ¼å¼
        current_message = messages[-1].content if messages else ""
        conversation_history = [
            {"role": "user" if isinstance(m, HumanMessage) else "assistant", "content": m.content}
            for m in messages[:-1]
        ]
        
        # è°ƒç”¨ LLMService çš„æµå¼ç”Ÿæˆå™¨
        stream_generator = self.llm_service.chat_stream(
            message=current_message,
            conversation_history=conversation_history,
            session_id=kwargs.get("session_id") 
        )
        
        # è¿­ä»£ LLM äº§ç”Ÿçš„å¢é‡ï¼Œå¹¶åŒ…è£…æˆ LangChain Chunk
        async for chunk_content in stream_generator:
            # åˆ›å»º ChatGenerationChunk
            chunk = ChatGenerationChunk(
                text=chunk_content,
                message=AIMessage(content=chunk_content),
                # å¯é€‰ï¼šå¦‚æœéœ€è¦ LangChain Callbackï¼Œå¯ä»¥åœ¨è¿™é‡Œè°ƒç”¨ run_manager.on_llm_new_token(chunk_content)
            )
            yield chunk

    @property
    def _llm_type(self) -> str:
        return "vllm-qwen-chat-model"

# ----------------------------------------------------
# 2. æ ¸å¿ƒ LLMService ç±»
# ----------------------------------------------------
class LLMService:
    """LLMæœåŠ¡ - ä¸“é—¨é€‚é…vLLMçš„Qwenæ¨¡å‹"""
    
    def __init__(self):
        self.model_url = settings.LOCAL_MODEL_URL
        self.timeout = settings.MODEL_TIMEOUT
        self.max_tokens = settings.MODEL_MAX_TOKENS
        self.temperature = settings.MODEL_TEMPERATURE
        self.client = httpx.AsyncClient(timeout=self.timeout)
        self.is_ready = False
        self.langchain_llm = VLLMChatModel(llm_service=self)
    
    async def health_check(self) -> bool:
        
        try:
            health_url = f"http://{settings.LOCAL_MODEL_HOST}:{settings.LOCAL_MODEL_PORT}/health"
            response = await self.client.get(health_url)
            self.is_ready = response.status_code == 200
            logger.info(f"ğŸ¤– æ¨¡å‹æœåŠ¡çŠ¶æ€: {'æ­£å¸¸' if self.is_ready else 'å¼‚å¸¸'}")
            return self.is_ready
        except Exception as e:
            logger.warning(f"æ¨¡å‹æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            self.is_ready = False
            return False
        
    async def chat(self, message: str, session_id: Optional[str] = None, conversation_history: Optional[List[Dict]] = None) -> SimpleChatResponse:
  
        full_answer = ""
        start_time = time.time()
        try:
            async for chunk in self.chat_stream(message, session_id, conversation_history):
                full_answer += chunk
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… æ¨¡å‹å“åº”æˆåŠŸ (åŒæ­¥èšåˆ) - è€—æ—¶: {processing_time:.2f}s, å­—ç¬¦æ•°: {len(full_answer)}")
            return SimpleChatResponse(answer=full_answer, session_id=session_id, processing_time=processing_time)
        except Exception as e:
            logger.error(f"âŒ chat åŒæ­¥èšåˆå¤±è´¥: {e}")
            return SimpleChatResponse(answer="åŒæ­¥èŠå¤©å¤±è´¥ã€‚", session_id=session_id, processing_time=time.time() - start_time)


    async def chat_stream(
        self, 
        message: str, 
        session_id: Optional[str] = None,
        conversation_history: Optional[List[Dict]] = None
    ) -> AsyncIterator[str]:
        
        try:
            # æ„å»ºæ¶ˆæ¯å†å²
            messages = self._build_messages(message, conversation_history)
            
    
            payload = {
                "model": "qwen",
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "stream": True  
            }
            
            logger.debug(f"è°ƒç”¨vLLMæ¨¡å‹ (STREAM) - Session: {session_id}")
            
           
            async with self.client.stream(
                "POST",
                self.model_url,
                json=payload,
                timeout=self.timeout
            ) as response:
                
                response.raise_for_status() # æ£€æŸ¥åˆå§‹ HTTP çŠ¶æ€ç 

                # å¼‚æ­¥è¿­ä»£å“åº”è¡Œï¼ˆvLLM/OpenAI å…¼å®¹ SSE æ ¼å¼ï¼‰
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:].strip()
                        
                        if data == "[DONE]":
                            break # æµç»“æŸæ ‡è®°
                        
                        try:
                            # è§£æ JSON æ•°æ®å—
                            chunk = json.loads(data)
                            
                            # æå–å¢é‡æ–‡æœ¬: choices[0].delta.content
                            content = chunk.get("choices", [{}])[0].get("delta", {}).get("content")
                            
                            if content:
                                # ç”Ÿæˆ (yield) æ–‡æœ¬å¢é‡
                                yield content
                                
                        except json.JSONDecodeError:
                            logger.warning(f"æ— æ³•è§£æçš„æµå¼æ•°æ®å—: {data}")
                        except Exception as e:
                            logger.error(f"å¤„ç†æµå¼æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                            
        except httpx.HTTPStatusError as e:
            error_msg = f"æ¨¡å‹æœåŠ¡è¿”å› HTTP é”™è¯¯: {e.response.status_code}"
            logger.error(f"âŒ {error_msg}, å“åº”: {e.response.text[:100]}...")
            yield error_msg
        except httpx.TimeoutException:
            error_msg = f"æ¨¡å‹è°ƒç”¨è¶…æ—¶ - è¶…æ—¶è®¾ç½®: {self.timeout}s"
            logger.error(f"âŒ {error_msg}")
            yield error_msg
        except Exception as e:
            error_msg = f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}"
            logger.error(f"âŒ {error_msg}")
            yield error_msg


    def _build_messages(
        self, 
        current_message: str, 
        conversation_history: Optional[List[Dict]] = None
    ) -> List[Dict[str, str]]:
        """æ„å»ºOpenAIæ ¼å¼çš„æ¶ˆæ¯å†å² - ä¿æŒä¸å˜"""
        messages = []
        
        system_message = {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
        }
        messages.append(system_message)
        
        if conversation_history:
            for msg in conversation_history[-6:]:
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", "")
                })
        
        messages.append({
            "role": "user",
            "content": current_message
        })
        
        return messages
    
    def _parse_vllm_response(self, result: Dict[str, Any]) -> str:
        """è§£ævLLMçš„å“åº”æ ¼å¼ - ä¿æŒä¸å˜ (ä¸»è¦ç”¨äºåŒæ­¥ chat æ–¹æ³•çš„å…¼å®¹)"""
        try:
            if "choices" in result and len(result["choices"]) > 0:
                choice = result["choices"][0]
                if "message" in choice and "content" in choice["message"]:
                    return choice["message"]["content"].strip()
            
            logger.warning(f"éæ ‡å‡†å“åº”æ ¼å¼: {result}")
            
            if "text" in result:
                return result["text"].strip()
            elif "generated_text" in result:
                return result["generated_text"].strip()
            else:
                return f"[è°ƒè¯•] å“åº”æ ¼å¼å¼‚å¸¸: {str(result)[:200]}"
                
        except Exception as e:
            logger.error(f"è§£ææ¨¡å‹å“åº”å¤±è´¥: {e}, åŸå§‹å“åº”: {result}")
            return "æŠ±æ­‰ï¼Œæ¨¡å‹è¿”å›äº†æ— æ³•è§£æçš„å“åº”ã€‚"
    
    async def close(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯ - ä¿æŒä¸å˜"""
        await self.client.aclose()

# å…¨å±€LLMæœåŠ¡å®ä¾‹
llm_service = LLMService()